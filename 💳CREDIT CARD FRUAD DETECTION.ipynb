{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 23498,
          "sourceType": "datasetVersion",
          "datasetId": 310
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "üí≥FRAUDFIGHTER: Detecting Credit Card Fraud Ac 97%",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menerucha/Fraud-Detection/blob/main/%F0%9F%92%B3CREDIT%20CARD%20FRUAD%20DETECTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'creditcardfraud:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F310%2F23498%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240724%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240724T131003Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2312e26a683990f5a59491c318fc23b1dee03a8499c7d833aa78394d060d55f8a0b534c34eaec0683906fedbd67748fa3622807e65485adfb2512f923f373c4db0c045ac09fc5058e3c95a9f16f992dda525f49ddff794dcdd3c6201be5fb73246f293bac35787e0833f083f4213ff521e8ffcc114a814bd0a41040aab72c2bcc7516d9e9ee08a98cd07ea016bd8e43a8ba5f6f9279b5d57738448db86bad4a9b2aeb3890c38f6b180cf17c0b8a91ee60ec2850aa42398aaa21b7fa095021f9aa7b52c65ec53ac02e1f8beb26ab83d693baf1b15d960f17ed507db05fcb1ec8baa8d6ed232d7175dcc66fcc5dd5f120842f1b410e0421352345f9f189a6aaa27'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "PCOXShfq3RDd"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset Information"
      ],
      "metadata": {
        "id": "lO-t0ZBm3RDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Context:\n",
        "Credit card companies need to detect fraudulent transactions to prevent customers from being charged for unauthorized purchases.\n",
        "\n",
        "#### Content:\n",
        "- **Dataset**: Transactions made by European cardholders in September 2013.\n",
        "- **Duration**: Two days, with 492 frauds out of 284,807 transactions.\n",
        "- **Class Imbalance**: Fraudulent transactions (positive class) account for 0.172% of all transactions.\n",
        "- **Features**:\n",
        "  - Numerical input variables resulting from PCA transformation.\n",
        "  - 'Time': Seconds elapsed between each transaction and the first transaction.\n",
        "  - 'Amount': Transaction amount, suitable for cost-sensitive learning.\n",
        "\n",
        "- **Target**:\n",
        "  - 'Class': Response variable, 1 for fraud, 0 otherwise.\n",
        "  \n",
        "#### Source:\n",
        "- The dataset has been collected and analyzed by Worldline and the Machine Learning Group (MLG) of Universit√© Libre de Bruxelles (ULB) as part of a research collaboration on big data mining and fraud detection.\n",
        "- More details on the current and past projects related to fraud detection are available on the [MLG website](http://mlg.ulb.ac.be) and [ResearchGate](https://www.researchgate.net/project/Fraud-detection-5).\n",
        "\n",
        "#### Recommendations:\n",
        "- Due to class imbalance, accuracy should be measured using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful.\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b56sicB33RDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "iaswb6pf3RDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:18.094005Z",
          "iopub.execute_input": "2024-07-11T03:50:18.094393Z",
          "iopub.status.idle": "2024-07-11T03:50:20.516927Z",
          "shell.execute_reply.started": "2024-07-11T03:50:18.09435Z",
          "shell.execute_reply": "2024-07-11T03:50:20.515816Z"
        },
        "trusted": true,
        "id": "NK93wTK23RDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file 'creditcard.csv' into a Pandas DataFrame named df\n",
        "df = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:20.518832Z",
          "iopub.execute_input": "2024-07-11T03:50:20.519313Z",
          "iopub.status.idle": "2024-07-11T03:50:25.231262Z",
          "shell.execute_reply.started": "2024-07-11T03:50:20.519282Z",
          "shell.execute_reply": "2024-07-11T03:50:25.230046Z"
        },
        "trusted": true,
        "id": "Lgzw-Nm53RDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame df\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:25.237024Z",
          "iopub.execute_input": "2024-07-11T03:50:25.237349Z",
          "iopub.status.idle": "2024-07-11T03:50:25.284886Z",
          "shell.execute_reply.started": "2024-07-11T03:50:25.237321Z",
          "shell.execute_reply": "2024-07-11T03:50:25.283781Z"
        },
        "trusted": true,
        "id": "QvN1Qytu3RDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a concise summary of the DataFrame df, including column names, non-null counts...\n",
        "df.info()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:25.286309Z",
          "iopub.execute_input": "2024-07-11T03:50:25.286675Z",
          "iopub.status.idle": "2024-07-11T03:50:25.323726Z",
          "shell.execute_reply.started": "2024-07-11T03:50:25.286644Z",
          "shell.execute_reply": "2024-07-11T03:50:25.322655Z"
        },
        "trusted": true,
        "id": "7UmG_dA-3RDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of the dataset (number of rows and columns)\n",
        "print('Shape Of The Dataset', df.shape)\n",
        "\n",
        "# Print the unique class categories in the 'Class' column\n",
        "print('Class Categories', df['Class'].unique())\n",
        "\n",
        "# Print the number of records with the class value 0 in the 'Class' column\n",
        "print('Number Of Records With The Class Value 0: ', (df.Class == 0).sum())\n",
        "\n",
        "# Print the number of records with the class value 1 in the 'Class' column\n",
        "print('Number Of Records With The Class Value 1: ', (df.Class == 1).sum())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:25.325275Z",
          "iopub.execute_input": "2024-07-11T03:50:25.325677Z",
          "iopub.status.idle": "2024-07-11T03:50:25.339434Z",
          "shell.execute_reply.started": "2024-07-11T03:50:25.325647Z",
          "shell.execute_reply": "2024-07-11T03:50:25.338361Z"
        },
        "trusted": true,
        "id": "g1-fBIFV3RDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a count plot to visualize the distribution of classes in the 'Class' column of the DataFrame df\n",
        "sns.countplot(x='Class', data=df)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:25.340706Z",
          "iopub.execute_input": "2024-07-11T03:50:25.341129Z",
          "iopub.status.idle": "2024-07-11T03:50:25.553344Z",
          "shell.execute_reply.started": "2024-07-11T03:50:25.341093Z",
          "shell.execute_reply": "2024-07-11T03:50:25.552194Z"
        },
        "trusted": true,
        "id": "ux_ZK3m73RDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n",
        "    ‚ö†Ô∏è Credit card fraud datasets, including this one, are typically highly imbalanced because occurrences of fraud are rare compared to normal transactions. In the next sections, we will explore effective strategies for handling this imbalance.\n",
        "</div>"
      ],
      "metadata": {
        "id": "8Ak1AQxc3RDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Features Selection"
      ],
      "metadata": {
        "id": "Mi29Feph3RDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation between the 'Class' column and the first 30 columns\n",
        "x = df.corr()['Class'][:30]\n",
        "x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:25.554739Z",
          "iopub.execute_input": "2024-07-11T03:50:25.555068Z",
          "iopub.status.idle": "2024-07-11T03:50:26.365792Z",
          "shell.execute_reply.started": "2024-07-11T03:50:25.55504Z",
          "shell.execute_reply": "2024-07-11T03:50:26.364634Z"
        },
        "trusted": true,
        "id": "E4x97Kzb3RDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation coefficients between the 'Class' column and the first 30 columns\n",
        "x = df.corr()['Class'][:30]\n",
        "\n",
        "# Create a bar plot to visualize the correlation of features with the target variable 'Class'\n",
        "x.plot.bar(figsize=(16, 9), title=\"Correlation Of Features With Target Variable\", grid=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:26.367315Z",
          "iopub.execute_input": "2024-07-11T03:50:26.368207Z",
          "iopub.status.idle": "2024-07-11T03:50:27.843391Z",
          "shell.execute_reply.started": "2024-07-11T03:50:26.368167Z",
          "shell.execute_reply": "2024-07-11T03:50:27.842004Z"
        },
        "trusted": true,
        "id": "ofGskRw43RDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n",
        "    üìå Some features exhibit a negligible correlation with the target variable and will be removed in the subsequent sections. First, we'll examine the intercorrelation among variables.\n",
        "</div>"
      ],
      "metadata": {
        "id": "8pTxG0tF3RDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with a specific size for the heatmap\n",
        "plt.figure(figsize=(16, 9))\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix of the DataFrame df\n",
        "sns.heatmap(df.corr())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:27.849458Z",
          "iopub.execute_input": "2024-07-11T03:50:27.850489Z",
          "iopub.status.idle": "2024-07-11T03:50:29.451241Z",
          "shell.execute_reply.started": "2024-07-11T03:50:27.850445Z",
          "shell.execute_reply": "2024-07-11T03:50:29.450056Z"
        },
        "trusted": true,
        "id": "GFuRYJBa3RDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n",
        "    üìå The only intercorrelated variable among others is the transaction Amount. However, this variable shows no correlation with the target variable, so it will also be removed.\n",
        "</div>"
      ],
      "metadata": {
        "id": "DrpC1LBP3RDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation coefficients between 'Class' and all columns\n",
        "y = df.corr()['Class']\n",
        "\n",
        "# Create a copy of the DataFrame df\n",
        "df2 = df.copy()\n",
        "\n",
        "# Iterate through columns and drop those with absolute correlation less than 0.13\n",
        "for i in df.columns:\n",
        "    if abs(y[i]) < 0.13:\n",
        "        df2.drop(columns=[i], inplace=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:29.452661Z",
          "iopub.execute_input": "2024-07-11T03:50:29.452996Z",
          "iopub.status.idle": "2024-07-11T03:50:30.62581Z",
          "shell.execute_reply.started": "2024-07-11T03:50:29.452969Z",
          "shell.execute_reply": "2024-07-11T03:50:30.624674Z"
        },
        "trusted": true,
        "id": "mxgngpZD3RDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n",
        "    üìåHere, we filter our dataset to keep only features with a correlation above 0.13.\n",
        "</div>"
      ],
      "metadata": {
        "id": "2e4RcuYO3RDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame df2\n",
        "df2.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:30.627109Z",
          "iopub.execute_input": "2024-07-11T03:50:30.627438Z",
          "iopub.status.idle": "2024-07-11T03:50:30.64513Z",
          "shell.execute_reply.started": "2024-07-11T03:50:30.627412Z",
          "shell.execute_reply": "2024-07-11T03:50:30.643949Z"
        },
        "trusted": true,
        "id": "KAhrjP-s3RDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with a specific size for the heatmap\n",
        "plt.figure(figsize=(16, 9))\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix of the DataFrame df2\n",
        "sns.heatmap(df2.corr(), annot=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:30.64662Z",
          "iopub.execute_input": "2024-07-11T03:50:30.647034Z",
          "iopub.status.idle": "2024-07-11T03:50:31.467251Z",
          "shell.execute_reply.started": "2024-07-11T03:50:30.646996Z",
          "shell.execute_reply": "2024-07-11T03:50:31.466048Z"
        },
        "trusted": true,
        "id": "n8z92_wk3RDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation coefficients between the 'Class' column\n",
        "x = df2.corr()['Class'][:9]\n",
        "\n",
        "# Create a bar plot to visualize the top correlated features with the target variable 'Class'\n",
        "x.plot.bar(figsize=(16, 9), title=\"Top Correlated Features With The Target Variable\", grid=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:31.468665Z",
          "iopub.execute_input": "2024-07-11T03:50:31.468986Z",
          "iopub.status.idle": "2024-07-11T03:50:31.93607Z",
          "shell.execute_reply.started": "2024-07-11T03:50:31.468959Z",
          "shell.execute_reply": "2024-07-11T03:50:31.934775Z"
        },
        "trusted": true,
        "id": "kcnHOTPa3RDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Handling Data Imbalance"
      ],
      "metadata": {
        "id": "MtmqHNRj3RDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- This dataset consists of:\n",
        "  - Number of records with the class value 0: 284,315\n",
        "  - Number of records with the class value 1: 492\n",
        "\n",
        "Using this dataset as it is would be a fatal mistake due to its severe class imbalance. Here's why:\n",
        "\n",
        "- **Using the data as it is:**\n",
        "  - The overwhelming majority of records belong to the non-fraudulent class (class 0), making up over 99% of the dataset.\n",
        "  - Models trained on imbalanced data may prioritize accuracy on the majority class while neglecting the minority class (fraudulent transactions). This can result in poor performance in detecting fraud.\n",
        "\n",
        "- **Why oversampling is a fatal mistake:**\n",
        "  - Oversampling techniques like SMOTE (Synthetic Minority Over-sampling Technique) artificially inflate the minority class by generating synthetic examples. However, this can lead to overfitting and the introduction of noise, especially in cases where the minority class is already sparsely represented.\n",
        "\n",
        "- **Why downsampling is the best option:**\n",
        "  - Downsampling involves randomly reducing the number of samples in the majority class to balance it with the minority class. This approach helps mitigate the biases towards the majority class while maintaining the integrity of the dataset.\n",
        "  - By reducing the number of majority class samples to match the minority class, downsampling encourages the model to learn from both classes equally, improving its ability to accurately detect fraudulent transactions.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zgzzr3jP3RDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Using Imbalanced Data (No Sampling) Example:**\n",
        "   - Dataset:\n",
        "     - Class 0 (non-fraudulent transactions): 284,315 records\n",
        "     - Class 1 (fraudulent transactions): 492 records\n",
        "   - Example:\n",
        "     - Accuracy on test set: 99.8%\n",
        "     - Confusion Matrix:\n",
        "       ```\n",
        "                 Predicted Non-Fraudulent    Predicted Fraudulent\n",
        "       Actual Non-Fraudulent      71,078               200\n",
        "       Actual Fraudulent              50                40\n",
        "       ```\n",
        "     - Issue: High accuracy is misleading; the model fails to detect most fraudulent transactions (low recall for class 1).\n",
        "\n",
        "2. **Oversampling (SMOTE) Example:**\n",
        "   - Dataset:\n",
        "     - Original Class 0: 284,315 records\n",
        "     - Class 1: 492 records\n",
        "     - After SMOTE (oversampling Class 1 to match Class 0):\n",
        "       - Class 0: 284,315 records\n",
        "       - Class 1: 284,315 records (synthetic)\n",
        "   - Example:\n",
        "     - Model Performance:\n",
        "       - Accuracy: 98.5%\n",
        "       - Confusion Matrix:\n",
        "         ```\n",
        "                   Predicted Non-Fraudulent    Predicted Fraudulent\n",
        "         Actual Non-Fraudulent      70,800                  478\n",
        "         Actual Fraudulent              10                   80\n",
        "         ```\n",
        "     - Issue: High accuracy but high false positives due to synthetic examples, leading to overfitting and reduced precision for fraud detection."
      ],
      "metadata": {
        "id": "elGaCcf_3RDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n",
        "    üìå Having recognized why downsampling is the optimal technique for this dataset, let's proceed with downsampling our dataset.\n",
        "</div>"
      ],
      "metadata": {
        "id": "_3aNmsve3RDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df2.drop('Class', axis=1)\n",
        "y = df2['Class']\n",
        "\n",
        "# Initialize RandomUnderSampler\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "\n",
        "# Fit and apply the resampler to the data\n",
        "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "\n",
        "# Convert the resampled data back to a DataFrame\n",
        "downsampled_df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Class'])], axis=1)\n",
        "\n",
        "\n",
        "downsampled_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:31.937407Z",
          "iopub.execute_input": "2024-07-11T03:50:31.937763Z",
          "iopub.status.idle": "2024-07-11T03:50:32.575817Z",
          "shell.execute_reply.started": "2024-07-11T03:50:31.937734Z",
          "shell.execute_reply": "2024-07-11T03:50:32.574605Z"
        },
        "trusted": true,
        "id": "3TnECr6v3RDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the shape of the downsampled DataFrame downsampled_df\n",
        "downsampled_df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:32.577408Z",
          "iopub.execute_input": "2024-07-11T03:50:32.577907Z",
          "iopub.status.idle": "2024-07-11T03:50:32.584853Z",
          "shell.execute_reply.started": "2024-07-11T03:50:32.577877Z",
          "shell.execute_reply": "2024-07-11T03:50:32.583531Z"
        },
        "trusted": true,
        "id": "aehgFEEL3RDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Outliers?"
      ],
      "metadata": {
        "id": "4LF765zw3RDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a count plot to visualize the distribution of classes in the 'Class' column of the downsampled DataFrame downsampled_df\n",
        "sns.countplot(x='Class', data=downsampled_df)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:32.586281Z",
          "iopub.execute_input": "2024-07-11T03:50:32.586659Z",
          "iopub.status.idle": "2024-07-11T03:50:32.751084Z",
          "shell.execute_reply.started": "2024-07-11T03:50:32.586632Z",
          "shell.execute_reply": "2024-07-11T03:50:32.749874Z"
        },
        "trusted": true,
        "id": "UFxsp7Hh3RDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting using seaborn scatterplot\n",
        "sns.scatterplot(x='V11', y='V17', hue='Class', data=df2)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:32.752789Z",
          "iopub.execute_input": "2024-07-11T03:50:32.754191Z",
          "iopub.status.idle": "2024-07-11T03:50:49.527898Z",
          "shell.execute_reply.started": "2024-07-11T03:50:32.754146Z",
          "shell.execute_reply": "2024-07-11T03:50:49.526478Z"
        },
        "trusted": true,
        "id": "H3yDQscd3RDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# To ignore all warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Pair Plot of all variables\n",
        "sns.pairplot(downsampled_df, hue='Class')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:50:49.529334Z",
          "iopub.execute_input": "2024-07-11T03:50:49.529783Z",
          "iopub.status.idle": "2024-07-11T03:51:31.287142Z",
          "shell.execute_reply.started": "2024-07-11T03:50:49.529735Z",
          "shell.execute_reply": "2024-07-11T03:51:31.28551Z"
        },
        "trusted": true,
        "id": "UsYlwxp33RDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"font-size:14px; font-family:verdana;\">\n",
        "     ‚ö†Ô∏è Fraudulent transactions do not exhibit clustering behavior and do not conform to a normal distribution, making outlier identification challenging.\n",
        "    \n",
        "\n",
        "<br>- This phenomenon arises because fraud does not adhere to a typical distribution pattern; in other words, fraudulent activities vary widely and do not consistently follow specific patterns.\n",
        "\n",
        "<br>- Fraudsters employ diverse methods that evolve over time. It's important to note that simply obtaining credit card details and full names is insufficient for completing transactions. Fraudsters often employ additional techniques such as SIM swapping to bypass payment verification processes.\n",
        "\n",
        "<br>- Due to these factors, datasets containing fraudulent transactions lack distinct clustering and do not adhere to a normal distribution.\n",
        "\n",
        "<br>For these reasons, no records will be deleted from fraudulent transactions. Additionally, it's important to consider that fraudulent transactions are rare, making each record valuable.\n",
        "\n",
        "<br>Note: Outliers can still be removed from normal transactions (non-fraudulent transactions).\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "tXVJ0Mfn3RDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Initial Exploration of Models Performance"
      ],
      "metadata": {
        "id": "m92Bi5uA3RDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n",
        "    üìå To quickly assess model performance on this dataset, we will use LazyPredict for an initial evaluation of results.\n",
        "</div>"
      ],
      "metadata": {
        "id": "mOohk4mW3RD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation\n",
        "!pip install lazypredict"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:51:31.289281Z",
          "iopub.execute_input": "2024-07-11T03:51:31.289673Z",
          "iopub.status.idle": "2024-07-11T03:51:47.527652Z",
          "shell.execute_reply.started": "2024-07-11T03:51:31.289642Z",
          "shell.execute_reply": "2024-07-11T03:51:47.526352Z"
        },
        "trusted": true,
        "id": "anoo9bhW3RD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "# Separate features and target\n",
        "x = downsampled_df.drop(columns= 'Class')\n",
        "y = downsampled_df['Class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Fit all models\n",
        "clf = LazyClassifier(predictions=True)\n",
        "models, predictions = clf.fit(x_train, x_test, y_train, y_test)\n",
        "models"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:51:47.529531Z",
          "iopub.execute_input": "2024-07-11T03:51:47.52992Z",
          "iopub.status.idle": "2024-07-11T03:51:51.565152Z",
          "shell.execute_reply.started": "2024-07-11T03:51:47.529889Z",
          "shell.execute_reply": "2024-07-11T03:51:51.563904Z"
        },
        "trusted": true,
        "id": "qToGgDfD3RD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. LabelSpreading Tuning: No Improvement"
      ],
      "metadata": {
        "id": "gfzl9t1R3RD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = downsampled_df.drop('Class', axis=1)\n",
        "y = downsampled_df['Class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the LabelSpreading model\n",
        "model = LabelSpreading()\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'kernel': ['knn', 'rbf'],  # Kernel function to use\n",
        "    'gamma': ['scale', 'auto', 0.1, 1.0],  # Kernel coefficient for 'rbf' and 'poly' kernels\n",
        "    'alpha': [0.1, 0.2, 0.5, 0.8],  # Clamping factor\n",
        "    'n_neighbors': [3, 5, 7]  # Number of neighbors to consider\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "\n",
        "# Perform grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Cross-validation Accuracy: {best_score:.2f}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print classification report on the test set\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:51:51.56686Z",
          "iopub.execute_input": "2024-07-11T03:51:51.567648Z",
          "iopub.status.idle": "2024-07-11T03:52:05.135888Z",
          "shell.execute_reply.started": "2024-07-11T03:51:51.567605Z",
          "shell.execute_reply": "2024-07-11T03:52:05.134656Z"
        },
        "trusted": true,
        "id": "gP1lVHFQ3RD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Ensemble Voting: Major Improvement"
      ],
      "metadata": {
        "id": "jfrRn_UZ3RD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#As previously explained, we will include the import statements in the code\n",
        "#to allow you to use or test different parts of the code separately if needed.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Separate features and target\n",
        "X = downsampled_df.drop(columns='Class')\n",
        "y = downsampled_df['Class']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the models\n",
        "label_spreading = LabelSpreading()\n",
        "label_propagation = LabelPropagation()\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Create an ensemble\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('label_spreading', label_spreading),\n",
        "    ('label_propagation', label_propagation),\n",
        "    ('xgb', xgb)\n",
        "], voting='hard')\n",
        "\n",
        "# Fit the ensemble to the training data\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ensemble.predict(X_test)\n",
        "\n",
        "# Evaluate the performance\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:52:05.137421Z",
          "iopub.execute_input": "2024-07-11T03:52:05.137842Z",
          "iopub.status.idle": "2024-07-11T03:52:05.60488Z",
          "shell.execute_reply.started": "2024-07-11T03:52:05.137809Z",
          "shell.execute_reply": "2024-07-11T03:52:05.603124Z"
        },
        "trusted": true,
        "id": "2ng_Bzee3RD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Enhancing the Ensemble: Tuning Parameters"
      ],
      "metadata": {
        "id": "UXyVClbZ3RD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Separate features and target\n",
        "X = downsampled_df.drop(columns='Class')\n",
        "y = downsampled_df['Class']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the models\n",
        "label_spreading = LabelSpreading()\n",
        "label_propagation = LabelPropagation()\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Create an ensemble\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('label_spreading', label_spreading),\n",
        "    ('label_propagation', label_propagation),\n",
        "    ('xgb', xgb)\n",
        "], voting='hard')\n",
        "\n",
        "# Define parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'label_spreading__gamma': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
        "    'label_propagation__gamma': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
        "    'xgb__n_estimators': [50, 100, 150],\n",
        "    'xgb__max_depth': [3, 5, 7],\n",
        "    'xgb__learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=ensemble, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=0)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Cross-Validation Score:\", best_score)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T03:52:05.606429Z",
          "iopub.execute_input": "2024-07-11T03:52:05.606802Z",
          "iopub.status.idle": "2024-07-11T04:02:28.814945Z",
          "shell.execute_reply.started": "2024-07-11T03:52:05.606771Z",
          "shell.execute_reply": "2024-07-11T04:02:28.813026Z"
        },
        "trusted": true,
        "id": "iX7yGmIr3RD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Minimizing False Negatives: A Crucial Step"
      ],
      "metadata": {
        "id": "BjEA8yEY3RD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"font-size:14px; font-family:verdana;\">\n",
        "     ‚ö†Ô∏è In credit card fraud detection, minimizing false negatives (fraudulent transactions classified as non-fraudulent) is crucial and often as important as accuracy, if not more so. Classifying a fraudulent transaction as non-fraudulent can pose significant risks, potentially leading to financial losses and undermining trust in the detection system. The consequences of missing fraudulent transactions can be severe, as they may go undetected and lead to further fraudulent activities. Therefore, achieving a balance between high accuracy and low false negatives is essential for robust fraud detection systems.\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "QQk_Vyq-3RD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"font-size:14px; font-family:verdana;\">\n",
        "     ‚ö†Ô∏è Example:\n",
        "    \n",
        "\n",
        "\n",
        "- Suppose a bank earns $1 profit per transaction, whether normal or fraudulent.\n",
        "- There are 100 transactions: 99 normal and 1 fraudulent.\n",
        "- Profit from 99 normal transactions: ( 99 x \\$1 = \\$99 )\n",
        "\n",
        "Now, consider the fraudulent transaction:\n",
        "- Fraudulent transaction amount: \\$100.\n",
        "- If misclassified as normal, bank earns: \\$1.\n",
        "- Actual loss to the bank: \\$100.\n",
        "\n",
        "In this case:\n",
        "- Bank's profit from normal transactions: \\$99.\n",
        "- Loss from misclassified fraudulent transaction: \\$100.\n",
        "\n",
        "Misclassifying the fraudulent transaction as normal means:\n",
        "- Bank's total profit calculation: \\( \\$99 + \\$1 = \\$100 \\).\n",
        "- Actual loss due to fraud: \\$100.\n",
        "\n",
        "Therefore, misclassifying one fraudulent transaction as normal would result in the bank losing all the profits earned from normal transactions, resulting in a net profit of \\$0.\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "ZqBV3NLN3RD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "X = downsampled_df.drop(columns='Class')\n",
        "y = downsampled_df['Class']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the models\n",
        "label_spreading = LabelSpreading()\n",
        "label_propagation = LabelPropagation()\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Create an ensemble\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('label_spreading', label_spreading),\n",
        "    ('label_propagation', label_propagation),\n",
        "    ('xgb', xgb)\n",
        "], voting='soft')  # Use soft voting for probability output\n",
        "\n",
        "# Fit the ensemble to the training data\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with probabilities\n",
        "y_prob = ensemble.predict_proba(X_test)[:, 1]  # Probability of being class 1 (fraud)\n",
        "\n",
        "# Adjust threshold\n",
        "threshold = 0.26  # threshold\n",
        "y_pred_adjusted = (y_prob >= threshold).astype(int)\n",
        "\n",
        "# Evaluate the performance\n",
        "print(\"Confusion Matrix with Adjusted Threshold:\")\n",
        "print(confusion_matrix(y_test, y_pred_adjusted))\n",
        "print(\"\\nClassification Report with Adjusted Threshold:\")\n",
        "print(classification_report(y_test, y_pred_adjusted))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T04:02:28.817034Z",
          "iopub.execute_input": "2024-07-11T04:02:28.817369Z",
          "iopub.status.idle": "2024-07-11T04:02:29.310226Z",
          "shell.execute_reply.started": "2024-07-11T04:02:28.81734Z",
          "shell.execute_reply": "2024-07-11T04:02:29.308407Z"
        },
        "trusted": true,
        "id": "Jv0wCiOU3RD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Adjusting Probability Thresholds for Better Detection"
      ],
      "metadata": {
        "id": "-hoxqAbw3RD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "X = downsampled_df.drop(columns='Class')\n",
        "y = downsampled_df['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "label_spreading = LabelSpreading(alpha=0.1, gamma=0.1, kernel='knn', n_neighbors=3)\n",
        "label_propagation = LabelPropagation()\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "FRAUDFIGHTER = VotingClassifier(estimators=[\n",
        "    ('label_spreading', label_spreading),\n",
        "    ('label_propagation', label_propagation),\n",
        "    ('xgb', xgb)\n",
        "], voting='soft')\n",
        "\n",
        "FRAUDFIGHTER.fit(X_train, y_train)\n",
        "\n",
        "y_prob = FRAUDFIGHTER.predict_proba(X_test)[:, 1]\n",
        "\n",
        "threshold = 0.371\n",
        "y_pred_adjusted = (y_prob >= threshold).astype(int)\n",
        "\n",
        "print(\"Confusion Matrix with Adjusted Threshold:\")\n",
        "print(confusion_matrix(y_test, y_pred_adjusted))\n",
        "print(\"\\nClassification Report with Adjusted Threshold:\")\n",
        "print(classification_report(y_test, y_pred_adjusted))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-11T04:02:29.312627Z",
          "iopub.execute_input": "2024-07-11T04:02:29.312985Z",
          "iopub.status.idle": "2024-07-11T04:02:29.727713Z",
          "shell.execute_reply.started": "2024-07-11T04:02:29.31295Z",
          "shell.execute_reply": "2024-07-11T04:02:29.725938Z"
        },
        "trusted": true,
        "id": "ZfAdZiH_3RD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ensemble method using three algorithms: LabelSpreading, LabelPropagation, XGBClassifier.\n",
        "- Utilization of tuned hyperparameters obtained previously.\n",
        "- Introduction of a new threshold parameter for predictions with probabilities (threshold = 0.371).\n",
        "\n",
        "FRAUDFIGHTER achieved a high accuracy of 97% with only 3 false negatives (FN). It's worth noting that the number of false positives (FP) in this model was also reduced from 3 to 2. The weighted average precision, recall, and F1 score all equaled 97%, demonstrating the robustness and effectiveness of this model.\n",
        "</div>"
      ],
      "metadata": {
        "id": "JKRm-sRO3RD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Conclusions"
      ],
      "metadata": {
        "id": "AHxlJzDi3RD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Effective Model Development:** Through meticulous data preprocessing, feature selection, and model optimization steps, we've developed a robust fraud detection model.\n",
        "   \n",
        "2. **Importance of Imbalance Handling:** Addressing the imbalance in the dataset was critical. Downsampling the majority class improved model performance by ensuring balanced representation of fraudulent and non-fraudulent transactions.\n",
        "\n",
        "3. **Model Performance:** Our final model, FRAUDFIGHTER, achieved an impressive accuracy of 97% with minimal false negatives and false positives. This underscores its capability to accurately detect fraudulent transactions.\n",
        "\n",
        "4. **Threshold Optimization:** Fine-tuning the threshold for probability predictions significantly enhanced the model's sensitivity in detecting fraudulent activities, reducing false negatives and improving overall accuracy.\n",
        "\n",
        "5. **Ensemble Approach:** Leveraging an ensemble method with carefully selected algorithms (LabelSpreading, LabelPropagation, XGBClassifier) proved effective in boosting predictive performance and robustness.\n",
        "\n",
        "6. **Real-world Application:** The methodologies and techniques applied here are crucial for real-world applications, where the cost of misclassifying fraudulent transactions can be substantial both financially and in terms of trust and customer satisfaction.\n",
        "\n",
        "7. **Future Directions:** Further enhancements could involve exploring advanced feature engineering techniques, integrating additional data sources, or deploying the model in a real-time environment to continuously improve fraud detection capabilities."
      ],
      "metadata": {
        "id": "inGbI73O3RD3"
      }
    }
  ]
}